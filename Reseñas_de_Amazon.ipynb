{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1kVlwnb6sQgur-_Ew5xpc9z7WAV-RFFFz",
      "authorship_tag": "ABX9TyPA8l6uYhbS2C4Oci+rOrge",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/churroxd8/nlp-notebooks/blob/main/Rese%C3%B1as_de_Amazon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# 1. Instalación y carga de librerías\n",
        "# =============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Descarga de recursos de NLTK\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Configuración para visualización de resultados\n",
        "plt.style.use('ggplot')\n",
        "pd.set_option('display.max_colwidth', 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho8-13pfoMGi",
        "outputId": "ac61def6-4a24-4988-98d8-b72647a0c228"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 2. Carga y preparación de los datos\n",
        "# =========================================================\n",
        "def load_and_prepare_data(file_path):\n",
        "  print(\"Cargando el dataset...\")\n",
        "  try:\n",
        "    # Intento estándar\n",
        "    df = pd.read_csv(file_path)\n",
        "  except:\n",
        "    # Fallback de encoding\n",
        "    df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "  # Identificamos las columnas que nos interesan\n",
        "  cols_map = {\n",
        "      'text': 'reviews.text',\n",
        "      'title': 'reviews.title',\n",
        "      'rating': 'reviews.rating'\n",
        "  }\n",
        "\n",
        "  # Verificamos y ajustamos el nombre de las columnas si difieren\n",
        "  for key, val in cols_map.items():\n",
        "    if val not in df.columns:\n",
        "      # Buscamos columnas similares\n",
        "      candidates = [c for c in df.columns if key in c.lower()]\n",
        "      if candidates:\n",
        "        cols_map[key] = candidates[0]\n",
        "      else:\n",
        "        print(f\"⚠️ Advertencia: No se encontraron columnas para '{key}'\")\n",
        "\n",
        "  print(f\"Usando columnas: {cols_map}\")\n",
        "\n",
        "  # Limpiamos los nulos\n",
        "  df = df.dropna(subset=[cols_map['text']]).copy()\n",
        "\n",
        "  # Procesamos el texto (Título + Cuerpo)\n",
        "  title_col = cols_map.get('title')\n",
        "  text_col = cols_map.get['text']\n",
        "\n",
        "  # Rellenamos vacíos con strings vacíos para concatenar\n",
        "  df[text_col] = df[text_col].fillna('')\n",
        "  if title_col:\n",
        "    df[title_col] = df[title_col].fillna('')\n",
        "    df['combined_raw'] = df[title_col].astype(str) + \". \" + df[text_col].astype(str)\n",
        "  else:\n",
        "    df['combined_raw'] = df[text_col].astype(str)\n",
        "\n",
        "  # Procesamos Rating (valor numérico)\n",
        "  rating_col = cols_map.get('rating')\n",
        "  if rating_col:\n",
        "    # Forzamos a numérico, los errores se convierten en NaN\n",
        "    df['rating_score'] = pd.to_numeric(df[rating_col], errors='coerce')\n",
        "    # Llenamos los ratings faltantes con la media\n",
        "    df = df.dropna(subset=['rating_score'])\n",
        "  else:\n",
        "    df['rating_score'] = np.nan\n",
        "\n",
        "  # 3. Limpieza del texto\n",
        "  def clean_text(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text) # Elimina URLs\n",
        "    text = re.sub(r'<.*?>', '', text) # Elimina etiquetas html\n",
        "    text = re.sub(r'[^a-z\\s\\.]', '', text) # Mantiene solo letras y signos de puntuación para separar frases\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Normaliza los espacios\n",
        "    return text\n",
        "\n",
        "  df['clean_text'] = df['combined_raw'].apply(clean_text)\n",
        "\n",
        "  # Nos aseguramos de tomar reseñas con contenido\n",
        "  df = df[df['clean_text'].str.len() > 3].copy()\n",
        "\n",
        "  print(f\"Dataset listo: {len(df)} reseñas. Rating promedio global: {df['rating_score'].mean:.2}\")\n",
        "  return df\n",
        "\n",
        "FILE_PATH = '/content/drive/MyDrive/amazon_reviews.csv'"
      ],
      "metadata": {
        "id": "qbyWbj0gp6cJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================\n",
        "# 3. Pipeline de extracción (Sustantivos)\n",
        "# ========================================================\n",
        "def extract_nouns_pipeline(df):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  # Stopwords para limpiar el ruido\n",
        "  domain_stopwords = {\n",
        "      'amazon',\n",
        "      'product',\n",
        "      'one',\n",
        "      'device',\n",
        "      'tablet',\n",
        "      'kindle',\n",
        "      'use',\n",
        "      'would',\n",
        "      'get',\n",
        "      'buy',\n",
        "      'item',\n",
        "      'review',\n",
        "      'star',\n",
        "      'purchase'\n",
        "  }\n",
        "  stop_words.update(domain_stopwords)\n",
        "\n",
        "  all_nouns = []\n",
        "\n",
        "  def process_row_nouns(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    row_nouns = []\n",
        "    for word, tag in tagged:\n",
        "      # Filtramos palabras cortas y stopwords\n",
        "      if len(word) > 2 and word not in stop_words:\n",
        "        # Solo sustantivos (NN, NNS)\n",
        "        if tag.startswith('NN'):\n",
        "          lemma = lemmatizer.lemmatize(word)\n",
        "          # Segunda revisión tras la lematización\n",
        "          if lemma not in domain_stopwords:\n",
        "            row_nouns.append(lemma)\n",
        "    return row_nouns\n",
        "\n",
        "  print(\"Ejecutando POS Tagging y extracción de sustantivos...\")\n",
        "  df['nouns'] = df['clean_text'].apply(process_row_nouns)\n",
        "\n",
        "  # Lista global para el conteo\n",
        "  all_nouns = [noun for row in df['nouns'] for noun in row]\n",
        "\n",
        "  return df, all_nouns\n",
        ""
      ],
      "metadata": {
        "id": "-W_8HF3fv1BC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# 4. Análisis de impacto de aspectos"
      ],
      "metadata": {
        "id": "cfUX0U5kyNa3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}